{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "class transformer_model(nn.Module):\n",
    "    def __init__(self, policy_config):\n",
    "        super(transformer_model, self).__init__()\n",
    "        # 初始化一个encoder\n",
    "        self.policy_config = policy_config\n",
    "        self.d_model = policy_config.get('d_model', 512)\n",
    "        self.nhead = policy_config.get('nhead', 6)\n",
    "        self.dim_feedforward = policy_config.get('dim_feedforward', 2048)\n",
    "        self.dropout = policy_config.get('dropout', 0.1)\n",
    "        self.activation = policy_config.get('activation', torch.functional.relu)\n",
    "        self.batch_first = True\n",
    "        self.layer_norm_eps = policy_config.get('layer_norm_eps',  1e-5)\n",
    "        # 定义encoder的层数\n",
    "        self.num_encoder_layers = policy_config.get('num_encoder_layers', 6)\n",
    "        transformer_encoder_layer = nn.TransformerEncoderLayer(self.d_model, self.nhead, self.dim_feedforward, self.dropout, self.activation, batch_first = self.batch_first)\n",
    "        encoder_norm = LayerNorm(self.d_model, eps=self.layer_norm_eps)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer, self.num_encoder_layers, encoder_norm)\n",
    "        # 定义解码器部分，这个地方是一个Pointer Network\n",
    "\n",
    "    def forward(self, src):\n",
    "        memory = self.transformer_encoder(src)\n",
    "        return memory\n",
    "\n",
    "\n",
    "class transformer_pointer_network_decoder(nn.Module):\n",
    "    def __init__(self, policy_config):\n",
    "        super(transformer_pointer_network_decoder, self).__init__()\n",
    "        self.policy_config = policy_config\n",
    "        self.d_model = policy_config.get('d_model', 512)\n",
    "        self.nhead = policy_config.get('nhead', 8)\n",
    "        self.dim_feedforward = policy_config.get('dim_feedforward', 2048)\n",
    "        self.dropout = policy_config.get('dropout', 0.1)\n",
    "        self.activation = policy_config.get('activation', torch.functional.relu)\n",
    "        self.batch_first = True\n",
    "        self.device = policy_config['device']\n",
    "        # 这个action dim其实就是用户数目 + 1\n",
    "        self.action_dim = policy_config['action_dim']\n",
    "        self.layer_norm_eps = policy_config.get('layer_norm_eps',  1e-5)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(self.d_model, self.nhead, self.dim_feedforward, self.dropout, self.activation, self.layer_norm_eps, batch_first=self.batch_first)   \n",
    "        decoder_norm = LayerNorm(self.d_model, eps=self.layer_norm_eps)\n",
    "        # 定义transformer解码器的层数\n",
    "        num_decoder_layers = self.policy_config.get('num_decoder_layers', 6)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "        # 这个所谓的最大解码次数指的是说，我最多循环多少次就要结束解码过程\n",
    "        self.max_decoder_time = self.policy_config['max_decoder_time']\n",
    "        self.init_decoder = nn.Parameter(torch.empty(1, self.d_model, 1))\n",
    "        self.affine_layer = nn.Linear(self.dim_feedforward, self.action_dim)\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def farward(self, tgt, memory, inference_mode= True, action_list=None):\n",
    "        if inference_mode:\n",
    "            assert action_list is None\n",
    "        else:\n",
    "            assert action_list is not None\n",
    "        batch_size = tgt.shape[0]\n",
    "        # 上面是说，在推断，采样阶段，是不会传入action list这个列表的，只有在训练阶段才会，并且要计算V值的。\n",
    "        mask = torch.zeros(batch_size, self.action_dim).bool().to(self.device)\n",
    "        terminate_batch = torch.zeros(batch_size, 1).bool().to(self.device)\n",
    "        repeat_init_decoder = self.init_decoder.repeat(batch_size, 1, 1)\n",
    "        decoder_input = repeat_init_decoder.clone()\n",
    "        for i in range(self.max_decoder_time):\n",
    "            # 通过网络直接输出结果\n",
    "            transformer_decoder = self.decoder(decoder_input, memory)\n",
    "            # 通过线性变换，降维\n",
    "            linear_output = self.affine_layer(transformer_decoder)\n",
    "            # 我们只取最后的那一个向量就可以了, batchsize * action dim\n",
    "            attention_vector = linear_output[:,-1,:]\n",
    "            attention_mask = torch.zeros_like(mask, dtype=torch.float)\n",
    "            attention_mask.masked_fill_(mask, float(\"-inf\"))\n",
    "            attention_vector += attention_mask\n",
    "            # 进行softmax操作，得到概率向量\n",
    "            attn = torch.softmax(attention_vector, dim=-1)\n",
    "            if inference_mode:\n",
    "                scheduling_index = action_list[:, i]\n",
    "                # 这个action向量是一个调度序列，直接从attn中取值\n",
    "            else:\n",
    "                dist = Categorical(attn)\n",
    "                scheduling_index = dist.sample()\n",
    "                # 将那些已经结束了的batch的action变成0\n",
    "                scheduling_index.masked_fill_(terminate_batch, 0)\n",
    "            prob = attn.gather(1, scheduling_index)\n",
    "            # 这里会出现log 0的情况，因此增加一个小数, 这是因为前面有的batch结束了，那么就是0，但是这里强制下一次再选出0，就会出现log 0了。\n",
    "            log_prob = torch.log(prob + self.eps)\n",
    "            # 将那些提前结束的batch的位置，将值变成0\n",
    "            log_prob.masked_fill_(terminate_batch, 0)\n",
    "            # 修改mask和is terminate mask矩阵\n",
    "            mask = mask.scatter(0, scheduling_index, True)\n",
    "            # 得到结束了的batch\n",
    "            terminate_batch = scheduling_index == 0 \n",
    "            # 构建下一次循环的输入\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self, policy_config):\n",
    "        super(model, self).__init__()\n",
    "        # 首先需要定义卷积层，嵌入层，将数据变成batch_size * seq_len * 512的形式\n",
    "        self.policy_config = policy_config\n",
    "        self.embedding_dim = self.policy_config.get('d_model', 512)\n",
    "        self.conv_channel = self.policy_config['conv_channel']\n",
    "        self.hidden_dim = self.policy_config['hidden_dim']\n",
    "        self.conv_layer = nn.Conv2d(self.conv_channel, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.linear_average_reward_head = nn.Linear(1, self.hidden_dim)\n",
    "        self.linear_scheduling_count_head = nn.Linear(1, self.hidden_dim)\n",
    "        self.embedding_layer = nn.Linear(3*self.hidden_dim, self.embedding_dim)\n",
    "        self.transformer_encoder = transformer_model(self.policy_config)\n",
    "\n",
    "    def forward(self, src, action_list, inference_mode=True):\n",
    "        # 首先这个src中包含了三个部分，信道矩阵部分，维度是batch size * user num * 32\n",
    "        # average reward 部分，维度是batch size * user num * 32\n",
    "        # 最后是到目前位置，各个用户调度了多少次构成的向量 batch size * user num * 1\n",
    "        channel_matrix = src['channel_matrix']\n",
    "        average_reward = src['average_reward']\n",
    "        scheduling_count = src['scheduling_count']\n",
    "        channel_output = torch.relu(self.conv_layer(channel_matrix).sqeeuze(1))\n",
    "        average_reward_output = torch.relu(self.linear_average_reward_head(average_reward))\n",
    "        scheduling_count_output = torch.relu(self.linear_scheduling_count_head(scheduling_count))\n",
    "        # 拼接在一起构成backbone\n",
    "        backbone = torch.cat([channel_output, average_reward_output, scheduling_count_output], -1)\n",
    "        embedding_output = torch.cat(self.embedding_layer(backbone))\n",
    "        # 送入到Transformer encoder\n",
    "        transformer_output = self.transformer_encoder(embedding_output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
